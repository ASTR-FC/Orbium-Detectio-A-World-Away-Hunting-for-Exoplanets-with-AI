# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DCEKkuhPi0W1HiaSFRdYnUvl_KoXJdGi
"""

import numpy as np
import seaborn as sns
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

import pandas as pd

# Load CSV: skip comment lines starting with '#'
df = pd.read_csv('drive/MyDrive/cumulative.csv', comment='#', engine='python')

# Quick check
print(df.shape)
df.head()

pd.set_option('display.max_rows',100)
pd.set_option('display.max_columns',100)

# Printing the shape of the train and test dataset

print(df.shape)
print('The shape of train data is: Row {} columns {}'.format(df.shape[0],df.shape[1]))

df.info()

#Checking for missing values

df.isnull().sum()

df['koi_tce_delivname'].value_counts()

df.drop(columns=['koi_teq_err1','koi_teq_err2','kepler_name','koi_tce_delivname'],inplace=True)

#Separiting the categorical features

categorical= df.select_dtypes(include =[object])
print("Categorical Features in DataSet:",categorical.shape[1])
print(categorical.columns)

# Filling the null values of numerical features with the respective mean

numerical=df.select_dtypes(include =[np.float64, np.int64])

from sklearn.impute import SimpleImputer

imputer= SimpleImputer(strategy='mean')
df_numerical=imputer.fit_transform(numerical)

df_numerical=pd.DataFrame(df_numerical,columns=numerical.columns)

df= pd.concat([df_numerical,categorical],axis=1)

df.head()

df['koi_disposition'].value_counts()

sns.countplot(df['koi_disposition'],palette='pastel')

def handle_outliers_kepler(df):
    # Select only numerical columns
    numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns

    df_clean = df.copy()

    for col in numerical_cols:
        Q1 = df_clean[col].quantile(0.25)
        Q3 = df_clean[col].quantile(0.75)
        IQR = Q3 - Q1

        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR

        # Clip outliers to the boundary values
        df_clean[col] = df_clean[col].clip(lower=lower_bound, upper=upper_bound)

    return df_clean

df_clean = handle_outliers_kepler(df)

from sklearn.model_selection import train_test_split
from imblearn.over_sampling import RandomOverSampler

# Define target
y = df['koi_disposition']

# Drop only the columns that exist
drop_cols = [col for col in ['kepid', 'kepoi_name', 'kepler_name', 'koi_disposition', 'koi_pdisposition'] if col in df.columns]
X = df.drop(columns=drop_cols)

# Step 1: Split the original data FIRST
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# Step 2: Apply oversampling ONLY to the training set
ros = RandomOverSampler(random_state=42)
X_train_res, y_train_res = ros.fit_resample(X_train, y_train)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score, StratifiedKFold
import numpy as np

#Random Forest 
rf = RandomForestClassifier(random_state=42)
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Perform cross-validation on the *resampled* training set
scores = cross_val_score(rf, X_train_res, y_train_res, cv=cv, scoring='accuracy')

print("Cross-validation scores:", scores)
print("Mean accuracy:", np.mean(scores))

# Fit the model on the oversampled training set
rf.fit(X_train_res, y_train_res)

# Predict on the untouched test set
y_pred = rf.predict(X_test)
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

test_accuracy = accuracy_score(y_test, y_pred)
print("Test set accuracy:", test_accuracy)

print("\nClassification Report:\n", classification_report(y_test, y_pred))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred))

"""XGBoost Test

"""

from sklearn.preprocessing import LabelEncoder

# Encode the target
le = LabelEncoder()
y_train_enc = le.fit_transform(y_train_res)
y_test_enc = le.transform(y_test)

# Now train the XGBoost model
xgb_model = xgb.XGBClassifier(
    objective='multi:softmax',  # multiclass
    num_class=len(le.classes_), # number of classes
    eval_metric='mlogloss',
    use_label_encoder=False,
    random_state=42
)

# Cross-validation
cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(xgb_model, X_train_res, y_train_enc, cv=cv, scoring='accuracy')
print("Cross-validation scores:", scores)
print("Mean CV accuracy:", np.mean(scores))

# Fit final model
xgb_model.fit(X_train_res, y_train_enc)

# Predict on test set
y_pred_enc = xgb_model.predict(X_test)
y_pred = le.inverse_transform(y_pred_enc)  # convert back to original labels

from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
print("Test set accuracy:", accuracy_score(y_test, y_pred))
print("Classification Report:\n", classification_report(y_test, y_pred))
print("Confusion Matrix:\n", confusion_matrix(y_test, y_pred))

import joblib
joblib.dump(xgb_model, 'xgboost_kepler_model.pkl')
print("Model saved successfully!")

import matplotlib.pyplot as plt
import numpy as np

# Plotlib
classes = ['CANDIDATE', 'CONFIRMED', 'FALSE POSITIVE']
precision_rf = [0.80, 0.89, 0.99]     # Random Forest
precision_xgb = [0.82, 0.90, 0.99]    # XGBoost
x = np.arange(len(classes))
width = 0.35
fig, ax = plt.subplots(figsize=(8,5))
rects1 = ax.bar(x - width/2, precision_rf, width, label='Random Forest', color='skyblue')
rects2 = ax.bar(x + width/2, precision_xgb, width, label='XGBoost', color='salmon'
ax.set_ylabel('Precision')
ax.set_title('Precision per Class: Random Forest vs XGBoost')
ax.set_xticks(x)
ax.set_xticklabels(classes)
ax.set_ylim(0,1.1)
ax.legend()
for rects in [rects1, rects2]:
    for rect in rects:
        height = rect.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(rect.get_x() + rect.get_width() / 2, height),
                    xytext=(0,3),  # 3 points vertical offset
                    textcoords="offset points",
                    ha='center', va='bottom')

plt.show()
